{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "baseDir = '/Users/linamiao/Downloads/'\n",
    "dfSmall = sqlContext.read.parquet(baseDir + 'smallwiki.parquet')\n",
    "print dfSmall.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfSmall: DataFrame[last_contributor_username: string, redirect_title: string, text: string, timestamp: string, title: string]\n"
     ]
    }
   ],
   "source": [
    "print 'dfSmall: {0}'.format(dfSmall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(dfSmall): <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print 'type(dfSmall): {0}'.format(type(dfSmall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(last_contributor_username,StringType,true),StructField(redirect_title,StringType,true),StructField(text,StringType,true),StructField(timestamp,StringType,true),StructField(title,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "print dfSmall.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- last_contributor_username: string (nullable = true)\n",
      " |-- redirect_title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSmall.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = false)\n",
      " |-- numberOfEdits: long (nullable = true)\n",
      " |-- redacted: boolean (nullable = true)\n",
      "\n",
      "+--------------------+-------------+--------+\n",
      "|               title|numberOfEdits|redacted|\n",
      "+--------------------+-------------+--------+\n",
      "|      Baade's Window|          100|   false|\n",
      "|             Zenomia|           10|    true|\n",
      "|United States Bur...|         5280|    true|\n",
      "+--------------------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, BooleanType, StringType, LongType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "schema = StructType([StructField('title', StringType(), nullable=False, metadata={'language': 'english'}),\n",
    "                     StructField('numberOfEdits', LongType()),\n",
    "                     StructField('redacted', BooleanType())])\n",
    "\n",
    "exampleData = sc.parallelize([Row(\"Baade's Window\", 100, False),\n",
    "                              Row('Zenomia', 10, True),\n",
    "                              Row('United States Bureau of Mines', 5280, True)])\n",
    "\n",
    "exampleDF = sqlContext.createDataFrame(exampleData, schema)\n",
    "exampleDF.printSchema()\n",
    "exampleDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print exampleDF.schema.fields[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(last_contributor_username=None, redirect_title=None, text=None, timestamp=u'2017-03-15T02:49:58Z', title=u'Adobe Systems')\n"
     ]
    }
   ],
   "source": [
    "print dfSmall.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['last_contributor_username', 'redirect_title', 'text', 'timestamp', 'title']\n"
     ]
    }
   ],
   "source": [
    "print dfSmall.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(last_contributor_username=None, redirect_title=None, timestamp=u'2017-03-15T02:49:58Z', title=u'Adobe Systems')\n"
     ]
    }
   ],
   "source": [
    "print dfSmall.drop('text').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print dfSmall.select('text').first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "errors = dfSmall.filter(col('title') == '<PARSE ERROR>')\n",
    "errorCount = errors.count()\n",
    "print errorCount\n",
    "print errorCount / float(dfSmall.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "errors = dfSmall.filter(col('title') == '<PARSE ERROR>')\n",
    "errorCount = errors.count()\n",
    "print errorCount\n",
    "print errorCount / float(dfSmall.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print dfSmall.filter(dfSmall['title'] == '<PARSE ERROR>').count()\n",
    "print dfSmall.filter(dfSmall.title == '<PARSE ERROR>').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|badTitle|\n",
      "+--------+\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "errors.select(col('title').alias('badTitle')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|hasRedirect|count|\n",
      "+-----------+-----+\n",
      "|       true|    4|\n",
      "|      false|    9|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(dfSmall\n",
    " .select(col('redirect_title').isNotNull().alias('hasRedirect'))\n",
    " .groupBy('hasRedirect')\n",
    " .count()\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "filtered = dfSmall.filter((col('title') != '<PARSE ERROR>') &\n",
    "                           col('redirect_title').isNull() &\n",
    "                           col('text').isNotNull())\n",
    "print filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AutoBatchedSerializer',\n",
       " 'Column',\n",
       " 'DataFrame',\n",
       " 'PickleSerializer',\n",
       " 'SparkContext',\n",
       " 'StringType',\n",
       " 'UserDefinedFunction',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '_binary_mathfunctions',\n",
       " '_create_binary_mathfunction',\n",
       " '_create_function',\n",
       " '_create_window_function',\n",
       " '_functions',\n",
       " '_functions_1_4',\n",
       " '_functions_1_6',\n",
       " '_functions_2_1',\n",
       " '_prepare_for_python_RDD',\n",
       " '_string_functions',\n",
       " '_test',\n",
       " '_to_java_column',\n",
       " '_to_seq',\n",
       " '_window_functions',\n",
       " '_wrap_function',\n",
       " 'abs',\n",
       " 'acos',\n",
       " 'add_months',\n",
       " 'approxCountDistinct',\n",
       " 'approx_count_distinct',\n",
       " 'array',\n",
       " 'array_contains',\n",
       " 'asc',\n",
       " 'ascii',\n",
       " 'asin',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'avg',\n",
       " 'base64',\n",
       " 'bin',\n",
       " 'bitwiseNOT',\n",
       " 'blacklist',\n",
       " 'broadcast',\n",
       " 'bround',\n",
       " 'cbrt',\n",
       " 'ceil',\n",
       " 'coalesce',\n",
       " 'col',\n",
       " 'collect_list',\n",
       " 'collect_set',\n",
       " 'column',\n",
       " 'concat',\n",
       " 'concat_ws',\n",
       " 'conv',\n",
       " 'corr',\n",
       " 'cos',\n",
       " 'cosh',\n",
       " 'count',\n",
       " 'countDistinct',\n",
       " 'covar_pop',\n",
       " 'covar_samp',\n",
       " 'crc32',\n",
       " 'create_map',\n",
       " 'cume_dist',\n",
       " 'current_date',\n",
       " 'current_timestamp',\n",
       " 'date_add',\n",
       " 'date_format',\n",
       " 'date_sub',\n",
       " 'datediff',\n",
       " 'dayofmonth',\n",
       " 'dayofyear',\n",
       " 'decode',\n",
       " 'degrees',\n",
       " 'dense_rank',\n",
       " 'desc',\n",
       " 'encode',\n",
       " 'exp',\n",
       " 'explode',\n",
       " 'expm1',\n",
       " 'expr',\n",
       " 'factorial',\n",
       " 'first',\n",
       " 'floor',\n",
       " 'format_number',\n",
       " 'format_string',\n",
       " 'from_json',\n",
       " 'from_unixtime',\n",
       " 'from_utc_timestamp',\n",
       " 'get_json_object',\n",
       " 'greatest',\n",
       " 'grouping',\n",
       " 'grouping_id',\n",
       " 'hash',\n",
       " 'hex',\n",
       " 'hour',\n",
       " 'hypot',\n",
       " 'ignore_unicode_prefix',\n",
       " 'initcap',\n",
       " 'input_file_name',\n",
       " 'instr',\n",
       " 'isnan',\n",
       " 'isnull',\n",
       " 'json_tuple',\n",
       " 'k',\n",
       " 'kurtosis',\n",
       " 'lag',\n",
       " 'last',\n",
       " 'last_day',\n",
       " 'lead',\n",
       " 'least',\n",
       " 'length',\n",
       " 'levenshtein',\n",
       " 'lit',\n",
       " 'locate',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log1p',\n",
       " 'log2',\n",
       " 'lower',\n",
       " 'lpad',\n",
       " 'ltrim',\n",
       " 'map',\n",
       " 'math',\n",
       " 'max',\n",
       " 'md5',\n",
       " 'mean',\n",
       " 'min',\n",
       " 'minute',\n",
       " 'monotonically_increasing_id',\n",
       " 'month',\n",
       " 'months_between',\n",
       " 'nanvl',\n",
       " 'next_day',\n",
       " 'ntile',\n",
       " 'percent_rank',\n",
       " 'posexplode',\n",
       " 'pow',\n",
       " 'quarter',\n",
       " 'radians',\n",
       " 'rand',\n",
       " 'randn',\n",
       " 'rank',\n",
       " 'regexp_extract',\n",
       " 'regexp_replace',\n",
       " 'repeat',\n",
       " 'reverse',\n",
       " 'rint',\n",
       " 'round',\n",
       " 'row_number',\n",
       " 'rpad',\n",
       " 'rtrim',\n",
       " 'second',\n",
       " 'sha1',\n",
       " 'sha2',\n",
       " 'shiftLeft',\n",
       " 'shiftRight',\n",
       " 'shiftRightUnsigned',\n",
       " 'signum',\n",
       " 'sin',\n",
       " 'since',\n",
       " 'sinh',\n",
       " 'size',\n",
       " 'skewness',\n",
       " 'sort_array',\n",
       " 'soundex',\n",
       " 'spark_partition_id',\n",
       " 'split',\n",
       " 'sqrt',\n",
       " 'stddev',\n",
       " 'stddev_pop',\n",
       " 'stddev_samp',\n",
       " 'struct',\n",
       " 'substring',\n",
       " 'substring_index',\n",
       " 'sum',\n",
       " 'sumDistinct',\n",
       " 'sys',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'toDegrees',\n",
       " 'toRadians',\n",
       " 'to_date',\n",
       " 'to_json',\n",
       " 'to_utc_timestamp',\n",
       " 'translate',\n",
       " 'trim',\n",
       " 'trunc',\n",
       " 'udf',\n",
       " 'unbase64',\n",
       " 'unhex',\n",
       " 'unix_timestamp',\n",
       " 'upper',\n",
       " 'v',\n",
       " 'var_pop',\n",
       " 'var_samp',\n",
       " 'variance',\n",
       " 'weekofyear',\n",
       " 'when',\n",
       " 'window',\n",
       " 'year']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as func\n",
    "dir(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           timestamp|\n",
      "+--------------------+\n",
      "|2017-03-15T02:49:58Z|\n",
      "|2017-03-14T11:27:49Z|\n",
      "|2017-03-11T02:42:19Z|\n",
      "|2017-01-21T22:57:26Z|\n",
      "|2017-01-10T02:55:21Z|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSmall.select('timestamp').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|           timestamp|      date|\n",
      "+--------------------+----------+\n",
      "|2017-03-15T02:49:58Z|03/14/2017|\n",
      "|2017-03-14T11:27:49Z|03/14/2017|\n",
      "|2017-03-11T02:42:19Z|03/10/2017|\n",
      "|2017-01-21T22:57:26Z|01/21/2017|\n",
      "|2017-01-10T02:55:21Z|01/09/2017|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSmall.select('timestamp',func.date_format('timestamp','MM/dd/yyyy').alias('date')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- last_contributor_username: string (nullable = true)\n",
      " |-- redirect_title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n",
      "+-----------------+--------+\n",
      "|            title|    date|\n",
      "+-----------------+--------+\n",
      "|    Adobe Systems|03/73/17|\n",
      "|Andrzej Sapkowski|03/73/17|\n",
      "| Plague (disease)|03/69/17|\n",
      "+-----------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "withDate = dfSmall.withColumn('date', func.date_format('timestamp','MM/DD/YY'))\n",
    "withDate.printSchema()\n",
    "withDate.select('title','date').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- last_contributor_username: string (nullable = true)\n",
      " |-- redirect_title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- cest_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "withCEST = withDate.withColumn('cest_time', func.from_utc_timestamp('timestamp','Europe/Amsterdam'))\n",
    "withCEST.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|timestamp           |cest_time            |\n",
      "+--------------------+---------------------+\n",
      "|2017-03-15T02:49:58Z|2017-03-14 20:49:58.0|\n",
      "|2017-03-14T11:27:49Z|2017-03-14 05:27:49.0|\n",
      "|2017-03-11T02:42:19Z|2017-03-10 19:42:19.0|\n",
      "+--------------------+---------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "withCEST.select('timestamp','cest_time').show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "lowerid = withCEST.select('*', func.lower(col('text')).alias('lowerText'))\n",
    "print lowerid.select('lowerText').first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['last_contributor_username', 'redirect_title', 'text', 'timestamp', 'title', 'date', 'cest_time', 'lowerText']\n"
     ]
    }
   ],
   "source": [
    "print lowerid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['last_contributor_username', 'redirect_title', 'title', 'cest_time', 'text'] \n",
      "\n",
      "\n",
      "+-------------------------+--------------+-----------------+--------------------+----+\n",
      "|last_contributor_username|redirect_title|            title|           cest_time|text|\n",
      "+-------------------------+--------------+-----------------+--------------------+----+\n",
      "|                     null|          null|    Adobe Systems|2017-03-14 20:49:...|null|\n",
      "|                     null|          null|Andrzej Sapkowski|2017-03-14 05:27:...|null|\n",
      "|           Bender the Bot|          null| Plague (disease)|2017-03-10 19:42:...|null|\n",
      "+-------------------------+--------------+-----------------+--------------------+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "parsed = (lowerid.drop('text')\n",
    "                 .drop('timestamp')\n",
    "                 .drop('date')\n",
    "                 .withColumnRenamed('lowerText','text'))\n",
    "print parsed.columns, '\\n\\n'\n",
    "print parsed.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "tokenizer = (RegexTokenizer()\n",
    "             .setInputCol('text')\n",
    "             .setOutputCol('words')\n",
    "             .setPattern('\\\\W+'))\n",
    "wordsDF = tokenizer.transform(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'all', u'six', u'less', u'being', u'indeed']\n"
     ]
    }
   ],
   "source": [
    "stopWords = set(sc.textFile('/Users/linamiao/playground/spark/stop_words.txt').collect())\n",
    "print [word for i, word in zip(range(5),stopWords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'cat', 'spark', 'hy-phen']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "stopWordsBroadCast = sc.broadcast(stopWords)\n",
    "\n",
    "def keepWord(word):\n",
    "    if len(word) < 3:\n",
    "        return False\n",
    "    \n",
    "    if word in stopWordsBroadCast.value:\n",
    "        return False\n",
    "    \n",
    "    if re.search(re.compile(r'[0-9_]'),word):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def removeWords(words):\n",
    "    return [word for word in words if keepWord(word)]\n",
    "\n",
    "removeWords(['test', 'cat', 'do343', '343', 'spark', 'the', 'and', 'hy-phen', 'under_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "removeWordsUDF = udf(removeWords,ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noStopWords = (wordsDF\n",
    "               .withColumn('noStopWords', removeWordsUDF(col('words')))\n",
    "               .drop('words')\n",
    "               .withColumnRenamed('noStopWords', 'words'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-f1248750e5fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwordList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisterTempTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordList'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwordGroupCount2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'select word, count(word) as counts from wordList group by word order by counts desc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwordGroupCount2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wordList' is not defined"
     ]
    }
   ],
   "source": [
    "wordList.registerTempTable('wordList')\n",
    "wordGroupCount2 = sqlContext.sql('select word, count(word) as counts from wordList group by word order by counts desc')\n",
    "wordGroupCount2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

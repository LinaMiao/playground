{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ArrayType',\n",
       " 'BooleanType',\n",
       " 'ByteType',\n",
       " 'DenseMatrix',\n",
       " 'DenseVector',\n",
       " 'DoubleType',\n",
       " 'IntegerType',\n",
       " 'Matrices',\n",
       " 'Matrix',\n",
       " 'MatrixUDT',\n",
       " 'QRDecomposition',\n",
       " 'SparseMatrix',\n",
       " 'SparseVector',\n",
       " 'StructField',\n",
       " 'StructType',\n",
       " 'UserDefinedType',\n",
       " 'Vector',\n",
       " 'VectorUDT',\n",
       " 'Vectors',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '_convert_to_vector',\n",
       " '_double_to_long_bits',\n",
       " '_format_float',\n",
       " '_format_float_list',\n",
       " '_have_scipy',\n",
       " '_test',\n",
       " '_vector_size',\n",
       " 'array',\n",
       " 'copy_reg',\n",
       " 'fast_pickle_array',\n",
       " 'newlinalg',\n",
       " 'np',\n",
       " 'scipy',\n",
       " 'since',\n",
       " 'struct',\n",
       " 'sys',\n",
       " 'zip']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib import linalg\n",
    "dir(linalg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import data types\n",
    "from pyspark.mllib.linalg import DenseVector, SparseVector, SparseMatrix, DenseMatrix, Vectors, Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function dense in module pyspark.mllib.linalg:\n",
      "\n",
      "dense(*elements)\n",
      "    Create a dense vector of 64-bit floats from a Python list or numbers.\n",
      "    \n",
      "    >>> Vectors.dense([1, 2, 3])\n",
      "    DenseVector([1.0, 2.0, 3.0])\n",
      "    >>> Vectors.dense(1.0, 2.0)\n",
      "    DenseVector([1.0, 2.0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Vectors.dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(denseVector): <class 'pyspark.mllib.linalg.DenseVector'>\n",
      "denseVector:[1.0,2.0,3.0]\n"
     ]
    }
   ],
   "source": [
    "denseVector = Vectors.dense([1,2,3])\n",
    "print 'type(denseVector): {0}'.format(type(denseVector))\n",
    "print 'denseVector:{0}'.format(denseVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denseVector.dot(denseVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7416573867739413"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectors.norm(denseVector,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([1.0, 4.0, 9.0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denseVector * denseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([6.0, 7.0, 8.0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5+denseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sparse in module pyspark.mllib.linalg:\n",
      "\n",
      "sparse(size, *args)\n",
      "    Create a sparse vector, using either a dictionary, a list of\n",
      "    (index, value) pairs, or two separate arrays of indices and\n",
      "    values (sorted by index).\n",
      "    \n",
      "    :param size: Size of the vector.\n",
      "    :param args: Non-zero entries, as a dictionary, list of tuples,\n",
      "                 or two sorted lists containing indices and values.\n",
      "    \n",
      "    >>> Vectors.sparse(4, {1: 1.0, 3: 5.5})\n",
      "    SparseVector(4, {1: 1.0, 3: 5.5})\n",
      "    >>> Vectors.sparse(4, [(1, 1.0), (3, 5.5)])\n",
      "    SparseVector(4, {1: 1.0, 3: 5.5})\n",
      "    >>> Vectors.sparse(4, [1, 3], [1.0, 5.5])\n",
      "    SparseVector(4, {1: 1.0, 3: 5.5})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Vectors.sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(sparseVector): <class 'pyspark.mllib.linalg.SparseVector'>\n",
      "\n",
      "sparseVector: (10,[2,7],[1.0,5.0])\n"
     ]
    }
   ],
   "source": [
    "sparseVector = Vectors.sparse(10,[2,7],[1.0,5.0])\n",
    "print 'type(sparseVector): {0}'.format(type(sparseVector))\n",
    "print '\\nsparseVector: {0}'.format(sparseVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SparseVector in module pyspark.mllib.linalg object:\n",
      "\n",
      "class SparseVector(Vector)\n",
      " |  A simple sparse vector class for passing data to MLlib. Users may\n",
      " |  alternatively pass SciPy's {scipy.sparse} data types.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SparseVector\n",
      " |      Vector\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |  \n",
      " |  __init__(self, size, *args)\n",
      " |      Create a sparse vector, using either a dictionary, a list of\n",
      " |      (index, value) pairs, or two separate arrays of indices and\n",
      " |      values (sorted by index).\n",
      " |      \n",
      " |      :param size: Size of the vector.\n",
      " |      :param args: Active entries, as a dictionary {index: value, ...},\n",
      " |        a list of tuples [(index, value), ...], or a list of strictly\n",
      " |        increasing indices and a list of corresponding values [index, ...],\n",
      " |        [value, ...]. Inactive entries are treated as zeros.\n",
      " |      \n",
      " |      >>> SparseVector(4, {1: 1.0, 3: 5.5})\n",
      " |      SparseVector(4, {1: 1.0, 3: 5.5})\n",
      " |      >>> SparseVector(4, [(1, 1.0), (3, 5.5)])\n",
      " |      SparseVector(4, {1: 1.0, 3: 5.5})\n",
      " |      >>> SparseVector(4, [1, 3], [1.0, 5.5])\n",
      " |      SparseVector(4, {1: 1.0, 3: 5.5})\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |  \n",
      " |  asML(self)\n",
      " |      Convert this vector to the new mllib-local representation.\n",
      " |      This does NOT copy the data; it copies references.\n",
      " |      \n",
      " |      :return: :py:class:`pyspark.ml.linalg.SparseVector`\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  dot(self, other)\n",
      " |      Dot product with a SparseVector or 1- or 2-dimensional Numpy array.\n",
      " |      \n",
      " |      >>> a = SparseVector(4, [1, 3], [3.0, 4.0])\n",
      " |      >>> a.dot(a)\n",
      " |      25.0\n",
      " |      >>> a.dot(array.array('d', [1., 2., 3., 4.]))\n",
      " |      22.0\n",
      " |      >>> b = SparseVector(4, [2], [1.0])\n",
      " |      >>> a.dot(b)\n",
      " |      0.0\n",
      " |      >>> a.dot(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]))\n",
      " |      array([ 22.,  22.])\n",
      " |      >>> a.dot([1., 2., 3.])\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      AssertionError: dimension mismatch\n",
      " |      >>> a.dot(np.array([1., 2.]))\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      AssertionError: dimension mismatch\n",
      " |      >>> a.dot(DenseVector([1., 2.]))\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      AssertionError: dimension mismatch\n",
      " |      >>> a.dot(np.zeros((3, 2)))\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      AssertionError: dimension mismatch\n",
      " |  \n",
      " |  norm(self, p)\n",
      " |      Calculates the norm of a SparseVector.\n",
      " |      \n",
      " |      >>> a = SparseVector(4, [0, 1], [3., -4.])\n",
      " |      >>> a.norm(1)\n",
      " |      7.0\n",
      " |      >>> a.norm(2)\n",
      " |      5.0\n",
      " |  \n",
      " |  numNonzeros(self)\n",
      " |      Number of nonzero elements. This scans all active values and count non zeros.\n",
      " |  \n",
      " |  squared_distance(self, other)\n",
      " |      Squared distance from a SparseVector or 1-dimensional NumPy array.\n",
      " |      \n",
      " |      >>> a = SparseVector(4, [1, 3], [3.0, 4.0])\n",
      " |      >>> a.squared_distance(a)\n",
      " |      0.0\n",
      " |      >>> a.squared_distance(array.array('d', [1., 2., 3., 4.]))\n",
      " |      11.0\n",
      " |      >>> a.squared_distance(np.array([1., 2., 3., 4.]))\n",
      " |      11.0\n",
      " |      >>> b = SparseVector(4, [2], [1.0])\n",
      " |      >>> a.squared_distance(b)\n",
      " |      26.0\n",
      " |      >>> b.squared_distance(a)\n",
      " |      26.0\n",
      " |      >>> b.squared_distance([1., 2.])\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      AssertionError: dimension mismatch\n",
      " |      >>> b.squared_distance(SparseVector(3, [1,], [1.0,]))\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      AssertionError: dimension mismatch\n",
      " |  \n",
      " |  toArray(self)\n",
      " |      Returns a copy of this SparseVector as a 1-dimensional NumPy array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  parse(s)\n",
      " |      Parse string representation back into the SparseVector.\n",
      " |      \n",
      " |      >>> SparseVector.parse(' (4, [0,1 ],[ 4.0,5.0] )')\n",
      " |      SparseVector(4, {0: 4.0, 1: 5.0})\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Vector:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from Vector:\n",
      " |  \n",
      " |  __UDT__ = VectorUDT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sparseVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__UDT__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__len__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'asML',\n",
       " 'dot',\n",
       " 'indices',\n",
       " 'norm',\n",
       " 'numNonzeros',\n",
       " 'parse',\n",
       " 'size',\n",
       " 'squared_distance',\n",
       " 'toArray',\n",
       " 'values']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sparseVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indices', 'size', 'values'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dir(sparseVector)) - set(dir(SparseVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class SparseVector(Vector):\n",
      "    \"\"\"\n",
      "    A simple sparse vector class for passing data to MLlib. Users may\n",
      "    alternatively pass SciPy's {scipy.sparse} data types.\n",
      "    \"\"\"\n",
      "    def __init__(self, size, *args):\n",
      "        \"\"\"\n",
      "        Create a sparse vector, using either a dictionary, a list of\n",
      "        (index, value) pairs, or two separate arrays of indices and\n",
      "        values (sorted by index).\n",
      "\n",
      "        :param size: Size of the vector.\n",
      "        :param args: Active entries, as a dictionary {index: value, ...},\n",
      "          a list of tuples [(index, value), ...], or a list of strictly\n",
      "          increasing indices and a list of corresponding values [index, ...],\n",
      "          [value, ...]. Inactive entries are treated as zeros.\n",
      "\n",
      "        >>> SparseVector(4, {1: 1.0, 3: 5.5})\n",
      "        SparseVector(4, {1: 1.0, 3: 5.5})\n",
      "        >>> SparseVector(4, [(1, 1.0), (3, 5.5)])\n",
      "        SparseVector(4, {1: 1.0, 3: 5.5})\n",
      "        >>> SparseVector(4, [1, 3], [1.0, 5.5])\n",
      "        SparseVector(4, {1: 1.0, 3: 5.5})\n",
      "        \"\"\"\n",
      "        self.size = int(size)\n",
      "        \"\"\" Size of the vector. \"\"\"\n",
      "        assert 1 <= len(args) <= 2, \"must pass either 2 or 3 arguments\"\n",
      "        if len(args) == 1:\n",
      "            pairs = args[0]\n",
      "            if type(pairs) == dict:\n",
      "                pairs = pairs.items()\n",
      "            pairs = sorted(pairs)\n",
      "            self.indices = np.array([p[0] for p in pairs], dtype=np.int32)\n",
      "            \"\"\" A list of indices corresponding to active entries. \"\"\"\n",
      "            self.values = np.array([p[1] for p in pairs], dtype=np.float64)\n",
      "            \"\"\" A list of values corresponding to active entries. \"\"\"\n",
      "        else:\n",
      "            if isinstance(args[0], bytes):\n",
      "                assert isinstance(args[1], bytes), \"values should be string too\"\n",
      "                if args[0]:\n",
      "                    self.indices = np.frombuffer(args[0], np.int32)\n",
      "                    self.values = np.frombuffer(args[1], np.float64)\n",
      "                else:\n",
      "                    # np.frombuffer() doesn't work well with empty string in older version\n",
      "                    self.indices = np.array([], dtype=np.int32)\n",
      "                    self.values = np.array([], dtype=np.float64)\n",
      "            else:\n",
      "                self.indices = np.array(args[0], dtype=np.int32)\n",
      "                self.values = np.array(args[1], dtype=np.float64)\n",
      "            assert len(self.indices) == len(self.values), \"index and value arrays not same length\"\n",
      "            for i in xrange(len(self.indices) - 1):\n",
      "                if self.indices[i] >= self.indices[i + 1]:\n",
      "                    raise TypeError(\n",
      "                        \"Indices %s and %s are not strictly increasing\"\n",
      "                        % (self.indices[i], self.indices[i + 1]))\n",
      "\n",
      "    def numNonzeros(self):\n",
      "        \"\"\"\n",
      "        Number of nonzero elements. This scans all active values and count non zeros.\n",
      "        \"\"\"\n",
      "        return np.count_nonzero(self.values)\n",
      "\n",
      "    def norm(self, p):\n",
      "        \"\"\"\n",
      "        Calculates the norm of a SparseVector.\n",
      "\n",
      "        >>> a = SparseVector(4, [0, 1], [3., -4.])\n",
      "        >>> a.norm(1)\n",
      "        7.0\n",
      "        >>> a.norm(2)\n",
      "        5.0\n",
      "        \"\"\"\n",
      "        return np.linalg.norm(self.values, p)\n",
      "\n",
      "    def __reduce__(self):\n",
      "        return (\n",
      "            SparseVector,\n",
      "            (self.size, self.indices.tostring(), self.values.tostring()))\n",
      "\n",
      "    @staticmethod\n",
      "    def parse(s):\n",
      "        \"\"\"\n",
      "        Parse string representation back into the SparseVector.\n",
      "\n",
      "        >>> SparseVector.parse(' (4, [0,1 ],[ 4.0,5.0] )')\n",
      "        SparseVector(4, {0: 4.0, 1: 5.0})\n",
      "        \"\"\"\n",
      "        start = s.find('(')\n",
      "        if start == -1:\n",
      "            raise ValueError(\"Tuple should start with '('\")\n",
      "        end = s.find(')')\n",
      "        if end == -1:\n",
      "            raise ValueError(\"Tuple should end with ')'\")\n",
      "        s = s[start + 1: end].strip()\n",
      "\n",
      "        size = s[: s.find(',')]\n",
      "        try:\n",
      "            size = int(size)\n",
      "        except ValueError:\n",
      "            raise ValueError(\"Cannot parse size %s.\" % size)\n",
      "\n",
      "        ind_start = s.find('[')\n",
      "        if ind_start == -1:\n",
      "            raise ValueError(\"Indices array should start with '['.\")\n",
      "        ind_end = s.find(']')\n",
      "        if ind_end == -1:\n",
      "            raise ValueError(\"Indices array should end with ']'\")\n",
      "        new_s = s[ind_start + 1: ind_end]\n",
      "        ind_list = new_s.split(',')\n",
      "        try:\n",
      "            indices = [int(ind) for ind in ind_list if ind]\n",
      "        except ValueError:\n",
      "            raise ValueError(\"Unable to parse indices from %s.\" % new_s)\n",
      "        s = s[ind_end + 1:].strip()\n",
      "\n",
      "        val_start = s.find('[')\n",
      "        if val_start == -1:\n",
      "            raise ValueError(\"Values array should start with '['.\")\n",
      "        val_end = s.find(']')\n",
      "        if val_end == -1:\n",
      "            raise ValueError(\"Values array should end with ']'.\")\n",
      "        val_list = s[val_start + 1: val_end].split(',')\n",
      "        try:\n",
      "            values = [float(val) for val in val_list if val]\n",
      "        except ValueError:\n",
      "            raise ValueError(\"Unable to parse values from %s.\" % s)\n",
      "        return SparseVector(size, indices, values)\n",
      "\n",
      "    def dot(self, other):\n",
      "        \"\"\"\n",
      "        Dot product with a SparseVector or 1- or 2-dimensional Numpy array.\n",
      "\n",
      "        >>> a = SparseVector(4, [1, 3], [3.0, 4.0])\n",
      "        >>> a.dot(a)\n",
      "        25.0\n",
      "        >>> a.dot(array.array('d', [1., 2., 3., 4.]))\n",
      "        22.0\n",
      "        >>> b = SparseVector(4, [2], [1.0])\n",
      "        >>> a.dot(b)\n",
      "        0.0\n",
      "        >>> a.dot(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]))\n",
      "        array([ 22.,  22.])\n",
      "        >>> a.dot([1., 2., 3.])\n",
      "        Traceback (most recent call last):\n",
      "            ...\n",
      "        AssertionError: dimension mismatch\n",
      "        >>> a.dot(np.array([1., 2.]))\n",
      "        Traceback (most recent call last):\n",
      "            ...\n",
      "        AssertionError: dimension mismatch\n",
      "        >>> a.dot(DenseVector([1., 2.]))\n",
      "        Traceback (most recent call last):\n",
      "            ...\n",
      "        AssertionError: dimension mismatch\n",
      "        >>> a.dot(np.zeros((3, 2)))\n",
      "        Traceback (most recent call last):\n",
      "            ...\n",
      "        AssertionError: dimension mismatch\n",
      "        \"\"\"\n",
      "\n",
      "        if isinstance(other, np.ndarray):\n",
      "            if other.ndim not in [2, 1]:\n",
      "                raise ValueError(\"Cannot call dot with %d-dimensional array\" % other.ndim)\n",
      "            assert len(self) == other.shape[0], \"dimension mismatch\"\n",
      "            return np.dot(self.values, other[self.indices])\n",
      "\n",
      "        assert len(self) == _vector_size(other), \"dimension mismatch\"\n",
      "\n",
      "        if isinstance(other, DenseVector):\n",
      "            return np.dot(other.array[self.indices], self.values)\n",
      "\n",
      "        elif isinstance(other, SparseVector):\n",
      "            # Find out common indices.\n",
      "            self_cmind = np.in1d(self.indices, other.indices, assume_unique=True)\n",
      "            self_values = self.values[self_cmind]\n",
      "            if self_values.size == 0:\n",
      "                return 0.0\n",
      "            else:\n",
      "                other_cmind = np.in1d(other.indices, self.indices, assume_unique=True)\n",
      "                return np.dot(self_values, other.values[other_cmind])\n",
      "\n",
      "        else:\n",
      "            return self.dot(_convert_to_vector(other))\n",
      "\n",
      "    def squared_distance(self, other):\n",
      "        \"\"\"\n",
      "        Squared distance from a SparseVector or 1-dimensional NumPy array.\n",
      "\n",
      "        >>> a = SparseVector(4, [1, 3], [3.0, 4.0])\n",
      "        >>> a.squared_distance(a)\n",
      "        0.0\n",
      "        >>> a.squared_distance(array.array('d', [1., 2., 3., 4.]))\n",
      "        11.0\n",
      "        >>> a.squared_distance(np.array([1., 2., 3., 4.]))\n",
      "        11.0\n",
      "        >>> b = SparseVector(4, [2], [1.0])\n",
      "        >>> a.squared_distance(b)\n",
      "        26.0\n",
      "        >>> b.squared_distance(a)\n",
      "        26.0\n",
      "        >>> b.squared_distance([1., 2.])\n",
      "        Traceback (most recent call last):\n",
      "            ...\n",
      "        AssertionError: dimension mismatch\n",
      "        >>> b.squared_distance(SparseVector(3, [1,], [1.0,]))\n",
      "        Traceback (most recent call last):\n",
      "            ...\n",
      "        AssertionError: dimension mismatch\n",
      "        \"\"\"\n",
      "        assert len(self) == _vector_size(other), \"dimension mismatch\"\n",
      "\n",
      "        if isinstance(other, np.ndarray) or isinstance(other, DenseVector):\n",
      "            if isinstance(other, np.ndarray) and other.ndim != 1:\n",
      "                raise Exception(\"Cannot call squared_distance with %d-dimensional array\" %\n",
      "                                other.ndim)\n",
      "            if isinstance(other, DenseVector):\n",
      "                other = other.array\n",
      "            sparse_ind = np.zeros(other.size, dtype=bool)\n",
      "            sparse_ind[self.indices] = True\n",
      "            dist = other[sparse_ind] - self.values\n",
      "            result = np.dot(dist, dist)\n",
      "\n",
      "            other_ind = other[~sparse_ind]\n",
      "            result += np.dot(other_ind, other_ind)\n",
      "            return result\n",
      "\n",
      "        elif isinstance(other, SparseVector):\n",
      "            result = 0.0\n",
      "            i, j = 0, 0\n",
      "            while i < len(self.indices) and j < len(other.indices):\n",
      "                if self.indices[i] == other.indices[j]:\n",
      "                    diff = self.values[i] - other.values[j]\n",
      "                    result += diff * diff\n",
      "                    i += 1\n",
      "                    j += 1\n",
      "                elif self.indices[i] < other.indices[j]:\n",
      "                    result += self.values[i] * self.values[i]\n",
      "                    i += 1\n",
      "                else:\n",
      "                    result += other.values[j] * other.values[j]\n",
      "                    j += 1\n",
      "            while i < len(self.indices):\n",
      "                result += self.values[i] * self.values[i]\n",
      "                i += 1\n",
      "            while j < len(other.indices):\n",
      "                result += other.values[j] * other.values[j]\n",
      "                j += 1\n",
      "            return result\n",
      "        else:\n",
      "            return self.squared_distance(_convert_to_vector(other))\n",
      "\n",
      "    def toArray(self):\n",
      "        \"\"\"\n",
      "        Returns a copy of this SparseVector as a 1-dimensional NumPy array.\n",
      "        \"\"\"\n",
      "        arr = np.zeros((self.size,), dtype=np.float64)\n",
      "        arr[self.indices] = self.values\n",
      "        return arr\n",
      "\n",
      "    def asML(self):\n",
      "        \"\"\"\n",
      "        Convert this vector to the new mllib-local representation.\n",
      "        This does NOT copy the data; it copies references.\n",
      "\n",
      "        :return: :py:class:`pyspark.ml.linalg.SparseVector`\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "        \"\"\"\n",
      "        return newlinalg.SparseVector(self.size, self.indices, self.values)\n",
      "\n",
      "    def __len__(self):\n",
      "        return self.size\n",
      "\n",
      "    def __str__(self):\n",
      "        inds = \"[\" + \",\".join([str(i) for i in self.indices]) + \"]\"\n",
      "        vals = \"[\" + \",\".join([str(v) for v in self.values]) + \"]\"\n",
      "        return \"(\" + \",\".join((str(self.size), inds, vals)) + \")\"\n",
      "\n",
      "    def __repr__(self):\n",
      "        inds = self.indices\n",
      "        vals = self.values\n",
      "        entries = \", \".join([\"{0}: {1}\".format(inds[i], _format_float(vals[i]))\n",
      "                             for i in xrange(len(inds))])\n",
      "        return \"SparseVector({0}, {{{1}}})\".format(self.size, entries)\n",
      "\n",
      "    def __eq__(self, other):\n",
      "        if isinstance(other, SparseVector):\n",
      "            return other.size == self.size and np.array_equal(other.indices, self.indices) \\\n",
      "                and np.array_equal(other.values, self.values)\n",
      "        elif isinstance(other, DenseVector):\n",
      "            if self.size != len(other):\n",
      "                return False\n",
      "            return Vectors._equals(self.indices, self.values, list(xrange(len(other))), other.array)\n",
      "        return False\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        inds = self.indices\n",
      "        vals = self.values\n",
      "        if not isinstance(index, int):\n",
      "            raise TypeError(\n",
      "                \"Indices must be of type integer, got type %s\" % type(index))\n",
      "\n",
      "        if index >= self.size or index < -self.size:\n",
      "            raise IndexError(\"Index %d out of bounds.\" % index)\n",
      "        if index < 0:\n",
      "            index += self.size\n",
      "\n",
      "        if (inds.size == 0) or (index > inds.item(-1)):\n",
      "            return 0.\n",
      "\n",
      "        insert_index = np.searchsorted(inds, index)\n",
      "        row_ind = inds[insert_index]\n",
      "        if row_ind == index:\n",
      "            return vals[insert_index]\n",
      "        return 0.\n",
      "\n",
      "    def __ne__(self, other):\n",
      "        return not self.__eq__(other)\n",
      "\n",
      "    def __hash__(self):\n",
      "        result = 31 + self.size\n",
      "        nnz = 0\n",
      "        i = 0\n",
      "        while i < len(self.values) and nnz < 128:\n",
      "            if self.values[i] != 0:\n",
      "                result = 31 * result + int(self.indices[i])\n",
      "                bits = _double_to_long_bits(self.values[i])\n",
      "                result = 31 * result + (bits ^ (bits >> 32))\n",
      "                nnz += 1\n",
      "            i += 1\n",
      "        return result\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print inspect.getsource(SparseVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsupported operand type(s) for +: 'SparseVector' and 'SparseVector'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sparseVector + sparseVector\n",
    "except TypeError as e:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparseVector.dot(sparseVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0990195135927845"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparseVector.norm(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0990195135927845"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectors.norm(sparseVector,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LabeledPoint in module pyspark.mllib.regression:\n",
      "\n",
      "class LabeledPoint(__builtin__.object)\n",
      " |  Class that represents the features and labels of a data point.\n",
      " |  \n",
      " |  :param label:\n",
      " |    Label for this data point.\n",
      " |  :param features:\n",
      " |    Vector of features for this point (NumPy array, list,\n",
      " |    pyspark.mllib.linalg.SparseVector, or scipy.sparse column matrix).\n",
      " |  \n",
      " |  .. note:: 'label' and 'features' are accessible as class attributes.\n",
      " |  \n",
      " |  .. versionadded:: 1.0.0\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, label, features)\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "help(LabeledPoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledPoint: (1992.0,[3.0,5.5,10.0])\n",
      "\n",
      "labeledPoint.features: [3.0,5.5,10.0]\n",
      "type(labeledPoint.features): <class 'pyspark.mllib.linalg.DenseVector'>\n",
      "\n",
      "labeledPoint.label: 1992.0\n",
      "type(labeledPoint.label): <type 'float'>\n"
     ]
    }
   ],
   "source": [
    "labeledPoint = LabeledPoint(1992, [3.0, 5.5, 10.0])\n",
    "print 'labeledPoint: {0}'.format(labeledPoint)\n",
    "\n",
    "print '\\nlabeledPoint.features: {0}'.format(labeledPoint.features)\n",
    "# Notice that feaures are being stored as a DenseVector\n",
    "print 'type(labeledPoint.features): {0}'.format(type(labeledPoint.features))\n",
    "\n",
    "print '\\nlabeledPoint.label: {0}'.format(labeledPoint.label)\n",
    "print 'type(labeledPoint.label): {0}'.format(type(labeledPoint.label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledPointSparse: (1992.0,(10,[0,1,2],[3.0,5.5,10.0]))\n",
      "\n",
      "labeledPoint.featuresSparse: (10,[0,1,2],[3.0,5.5,10.0])\n",
      "type(labeledPointSparse.features): <class 'pyspark.mllib.linalg.SparseVector'>\n"
     ]
    }
   ],
   "source": [
    "labeledPointSparse = LabeledPoint(1992, Vectors.sparse(10, {0: 3.0, 1:5.5, 2: 10.0}))\n",
    "print 'labeledPointSparse: {0}'.format(labeledPointSparse)\n",
    "\n",
    "print '\\nlabeledPoint.featuresSparse: {0}'.format(labeledPointSparse.features)\n",
    "print 'type(labeledPointSparse.features): {0}'.format(type(labeledPointSparse.features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Rating in module pyspark.mllib.recommendation:\n",
      "\n",
      "class Rating(collections.Rating)\n",
      " |  Represents a (user, product, rating) tuple.\n",
      " |  \n",
      " |  >>> r = Rating(1, 2, 5.0)\n",
      " |  >>> (r.user, r.product, r.rating)\n",
      " |  (1, 2, 5.0)\n",
      " |  >>> (r[0], r[1], r[2])\n",
      " |  (1, 2, 5.0)\n",
      " |  \n",
      " |  .. versionadded:: 1.2.0\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Rating\n",
      " |      collections.Rating\n",
      " |      __builtin__.tuple\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from collections.Rating:\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |      Return self as a plain tuple.  Used by copy and pickle.\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Exclude the OrderedDict from pickling\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return a nicely formatted representation string\n",
      " |  \n",
      " |  _asdict(self)\n",
      " |      Return a new OrderedDict which maps field names to their values\n",
      " |  \n",
      " |  _replace(_self, **kwds)\n",
      " |      Return a new Rating object replacing specified fields with new values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from collections.Rating:\n",
      " |  \n",
      " |  _make(cls, iterable, new=<built-in method __new__ of type object>, len=<built-in function len>) from __builtin__.type\n",
      " |      Make a new Rating object from a sequence or iterable\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from collections.Rating:\n",
      " |  \n",
      " |  __new__(_cls, user, product, rating)\n",
      " |      Create new instance of Rating(user, product, rating)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from collections.Rating:\n",
      " |  \n",
      " |  product\n",
      " |      Alias for field number 1\n",
      " |  \n",
      " |  rating\n",
      " |      Alias for field number 2\n",
      " |  \n",
      " |  user\n",
      " |      Alias for field number 0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from collections.Rating:\n",
      " |  \n",
      " |  _fields = ('user', 'product', 'rating')\n",
      " |  \n",
      " |  _is_namedtuple_ = True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from __builtin__.tuple:\n",
      " |  \n",
      " |  __add__(...)\n",
      " |      x.__add__(y) <==> x+y\n",
      " |  \n",
      " |  __contains__(...)\n",
      " |      x.__contains__(y) <==> y in x\n",
      " |  \n",
      " |  __eq__(...)\n",
      " |      x.__eq__(y) <==> x==y\n",
      " |  \n",
      " |  __ge__(...)\n",
      " |      x.__ge__(y) <==> x>=y\n",
      " |  \n",
      " |  __getattribute__(...)\n",
      " |      x.__getattribute__('name') <==> x.name\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __getslice__(...)\n",
      " |      x.__getslice__(i, j) <==> x[i:j]\n",
      " |      \n",
      " |      Use of negative indices is not supported.\n",
      " |  \n",
      " |  __gt__(...)\n",
      " |      x.__gt__(y) <==> x>y\n",
      " |  \n",
      " |  __hash__(...)\n",
      " |      x.__hash__() <==> hash(x)\n",
      " |  \n",
      " |  __iter__(...)\n",
      " |      x.__iter__() <==> iter(x)\n",
      " |  \n",
      " |  __le__(...)\n",
      " |      x.__le__(y) <==> x<=y\n",
      " |  \n",
      " |  __len__(...)\n",
      " |      x.__len__() <==> len(x)\n",
      " |  \n",
      " |  __lt__(...)\n",
      " |      x.__lt__(y) <==> x<y\n",
      " |  \n",
      " |  __mul__(...)\n",
      " |      x.__mul__(n) <==> x*n\n",
      " |  \n",
      " |  __ne__(...)\n",
      " |      x.__ne__(y) <==> x!=y\n",
      " |  \n",
      " |  __rmul__(...)\n",
      " |      x.__rmul__(n) <==> n*x\n",
      " |  \n",
      " |  count(...)\n",
      " |      T.count(value) -> integer -- return number of occurrences of value\n",
      " |  \n",
      " |  index(...)\n",
      " |      T.index(value, [start, [stop]]) -> integer -- return first index of value.\n",
      " |      Raises ValueError if the value is not present.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import Rating\n",
    "help(Rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function namedtuple in module collections:\n",
      "\n",
      "namedtuple(*args, **kwargs)\n",
      "    Returns a new subclass of tuple with named fields.\n",
      "    \n",
      "    >>> Point = namedtuple('Point', ['x', 'y'])\n",
      "    >>> Point.__doc__                   # docstring for the new class\n",
      "    'Point(x, y)'\n",
      "    >>> p = Point(11, y=22)             # instantiate with positional args or keywords\n",
      "    >>> p[0] + p[1]                     # indexable like a plain tuple\n",
      "    33\n",
      "    >>> x, y = p                        # unpack like a regular tuple\n",
      "    >>> x, y\n",
      "    (11, 22)\n",
      "    >>> p.x + p.y                       # fields also accessible by name\n",
      "    33\n",
      "    >>> d = p._asdict()                 # convert to a dictionary\n",
      "    >>> d['x']\n",
      "    11\n",
      "    >>> Point(**d)                      # convert from a dictionary\n",
      "    Point(x=11, y=22)\n",
      "    >>> p._replace(x=100)               # _replace() is like str.replace() but targets named fields\n",
      "    Point(x=100, y=22)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "help(namedtuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "address: Address(city='Boulder', state='CO')\n"
     ]
    }
   ],
   "source": [
    "Address = namedtuple('Address', ['city', 'state'])\n",
    "address = Address('Boulder', 'CO')\n",
    "print 'address: {0}'.format(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    city|state|\n",
      "+--------+-----+\n",
      "| Boulder|   CO|\n",
      "|New York|   NY|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF = sqlContext.createDataFrame([Address('Boulder', 'CO'), Address('New York', 'NY')])\n",
    "DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|label| features|\n",
      "+-----+---------+\n",
      "|   10|[1.0,2.0]|\n",
      "|   20|[1.5,2.2]|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LabelAndFeatures = namedtuple('LabelAndFeatures', ['label', 'features'])\n",
    "row1 = LabelAndFeatures(10, Vectors.dense([1.0, 2.0]))\n",
    "row2 = LabelAndFeatures(20, Vectors.dense([1.5, 2.2]))\n",
    "\n",
    "df = sqlContext.createDataFrame([row1, row2])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "denseVec = DenseVector([1.5,2.5,3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named test_helper",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-e2aa0469221a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtest_helper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mTest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massertEquals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenseVec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDenseVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'incorrect value for denseVec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named test_helper"
     ]
    }
   ],
   "source": [
    "from test_helper import Test\n",
    "Test.assertEquals(denseVec, DenseVector([1.5, 2.5, 3.0]), 'incorrect value for denseVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeledP = LabeledPoint(10,denseVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-3c43d3448b90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massertEquals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeledP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'(10.0,[1.5,2.5,3.0])'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'incorrect value for labeledP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Test' is not defined"
     ]
    }
   ],
   "source": [
    "Test.assertEquals(str(labeledP), '(10.0,[1.5,2.5,3.0])', 'incorrect value for labeledP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|first|\n",
      "+-----+\n",
      "|  1.0|\n",
      "|  1.5|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "firstElement = udf(lambda v: float(v[0]), DoubleType())\n",
    "\n",
    "df2 = df.select(firstElement('features').alias('first'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
